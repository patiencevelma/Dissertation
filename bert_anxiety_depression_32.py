# -*- coding: utf-8 -*-
"""BERT ANXIETY DEPRESSION 32

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bi7quTBbfEPlKBjMzi3ZdqCs1xe5Gfd
"""

!pip install transformers datasets torch pandas scikit-learn matplotlib seaborn

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

directory_path = "content/drive/MyDrive/EXCEL DATASETS"

import pandas as pd
import numpy as np
import re
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define file paths
anxiety_file = "/content/drive/MyDrive/EXCEL DATASETS/Anxiety DATASET.csv"
depression_file = "/content/drive/MyDrive/EXCEL DATASETS/Depression DATASET.csv"

# Load datasets
anxiety_df = pd.read_csv(anxiety_file)
depression_df = pd.read_csv(depression_file)

# Strip extra spaces from column names
anxiety_df.columns = anxiety_df.columns.str.strip()
depression_df.columns = depression_df.columns.str.strip()

# Define the questionnaire columns for the anxiety dataset (with Age and Gender)
anxiety_columns = [
    '1. Age', '2. Gender',
    '1. In a semester, how often you felt nervous, anxious or on edge due to academic pressure?',
    '2. In a semester, how often have you been unable to stop worrying about your academic affairs?',
    '3. In a semester, how often have you had trouble relaxing due to academic pressure?',
    '4. In a semester, how often have you been easily annoyed or irritated because of academic pressure?',
    '5. In a semester, how often have you worried too much about academic affairs?',
    '6. In a semester, how often have you been so restless due to academic pressure that it is hard to sit still?',
    '7. In a semester, how often have you felt afraid, as if something awful might happen?',
    'Anxiety Label'
]

# Define the questionnaire columns for the depression dataset.
# We remove '1. Age' and '2. Gender' since they are already present from the anxiety dataset.
depression_columns = [
    # '1. Age', '2. Gender',  <-- skipped on purpose
    '1. In a semester, how often have you had little interest or pleasure in doing things?',
    '2. In a semester, how often have you been feeling down, depressed or hopeless?',
    '3. In a semester, how often have you had trouble falling or staying asleep, or sleeping too much?',
    '4. In a semester, how often have you been feeling tired or having little energy?',
    '5. In a semester, how often have you had poor appetite or overeating?',
    '6. In a semester, how often have you been feeling bad about yourself - or that you are a failure or have let yourself or your family down?',
    '7. In a semester, how often have you been having trouble concentrating on things, such as reading the books or watching television?',
    '8. In a semester, how often have you moved or spoke too slowly for other people to notice? Or you\'ve been moving a lot more than usual because you\'ve been restless?',
    '9. In a semester, how often have you had thoughts that you would be better off dead, or of hurting yourself?',
    'Depression Label'
]

# Select only the desired columns
anxiety_selected = anxiety_df[anxiety_columns]
depression_selected = depression_df[depression_columns]

# Merge the datasets side by side (concatenate columns)
combined_df = pd.concat([anxiety_selected, depression_selected], axis=1)

print("Merged data preview:")
print(combined_df.head())
print("The shape of the DataFrame is:", combined_df.shape)

# Define mapping for questionnaire responses
response_map = {
    0: "Not at all",
    1: "Several days",
    2: "More than half the days",
    3: "Nearly every day"
}

# Define comprehensive label mapping
label_mapping = {
    # Anxiety labels
    'Severe Anxiety': 2,
    'Moderate Anxiety': 1,
    'Mild Anxiety': 0,
    'Minimal Anxiety': 0,
    'No Anxiety': 0,

    # Depression labels
    'Severe Depression': 2,
    'Moderately Severe Depression': 1,
    'Moderate Depression': 0,
    'Mild Depression': 0,
    'Minimal Depression': 0,
    'No Depression': 0
}

# Identify the questionnaire response columns.
# For anxiety: all columns after "1. Age" and "2. Gender" and before "Anxiety Label".
anxiety_response_cols = anxiety_columns[2:-1]  # Excludes Age, Gender, and Anxiety Label
# For depression: all columns before "Depression Label"
depression_response_cols = depression_columns[:-1]  # All except the final label column

# Apply the response mapping for anxiety columns
for col in anxiety_response_cols:
    combined_df[col] = combined_df[col].map(response_map)

# Apply the response mapping for depression columns
for col in depression_response_cols:
    combined_df[col] = combined_df[col].map(response_map)

# Apply the label mapping for the label columns
combined_df['Anxiety Label'] = combined_df['Anxiety Label'].map(label_mapping)
combined_df['Depression Label'] = combined_df['Depression Label'].map(label_mapping)

# Update the DataFrame column headers:
# Remove numeric prefixes and convert all names to uppercase
def clean_header(header):
    # Remove a leading numeric pattern ("number. ") if it exists, then strip extra whitespace
    header_no_num = re.sub(r'^[0-9]+\.\s*', '', header)
    return header_no_num.upper()

combined_df.columns = [clean_header(col) for col in combined_df.columns]

# Check the updated DataFrame header
print("Updated merged data preview with cleaned headers:")
print(combined_df.head())

# Save the updated DataFrame to Google Drive without the numbered index
output_file = "/content/drive/MyDrive/EXCEL DATASETS/COMBINED_ANXIETY_DEPRESSION_MAPPED2.csv"
combined_df.to_csv(output_file, index=False)
print("Mapped merged CSV with cleaned headers saved as:", output_file)

# First, let's set up the GPU environment
import torch

# Check if GPU is available and set device
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'Using GPU: {torch.cuda.get_device_name(0)}')
else:
    device = torch.device("cpu")
    print('No GPU available, using CPU instead')



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizer, BertConfig
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess data
def load_and_preprocess():
    df = pd.read_csv("/content/drive/MyDrive/EXCEL DATASETS/COMBINED_ANXIETY_DEPRESSION_MAPPED2.csv")

    # Combine text features
    anxiety_cols = [col for col in df.columns if 'ANXIETY' in col and 'LABEL' not in col]
    depression_cols = [col for col in df.columns if 'DEPRESSION' in col and 'LABEL' not in col]

    def create_text(row):
        anxiety_part = " ".join(f"A{i+1}:{row[col]}" for i,col in enumerate(anxiety_cols))
        depression_part = " ".join(f"D{i+1}:{row[col]}" for i,col in enumerate(depression_cols))
        return f"[AGE:{row['AGE']}] [GENDER:{row['GENDER']}] {anxiety_part} {depression_part}"

    df['text'] = df.apply(create_text, axis=1)

    # Verify labels
    print("Label distributions:")
    print("Anxiety:", df['ANXIETY LABEL'].value_counts())
    print("Depression:", df['DEPRESSION LABEL'].value_counts())

    return df

df = load_and_preprocess()

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizerFast

# 1) Load your merged CSV
csv_path = "/content/drive/MyDrive/EXCEL DATASETS/COMBINED_ANXIETY_DEPRESSION_MAPPED2.csv"
df = pd.read_csv(csv_path)

# 2) Combine question columns
meta = ["AGE","GENDER","ANXIETY LABEL","DEPRESSION LABEL"]
qs   = [c for c in df.columns if c not in meta]
df["text"] = df[qs].astype(str).agg(" . ".join, axis=1)

# 3) Rename labels
df = df.rename(columns={
    "ANXIETY LABEL": "anx_label",
    "DEPRESSION LABEL": "dep_label"
})

# 4) Train/test split
train_df, test_df = train_test_split(
    df[["text","anx_label","dep_label"]],
    test_size=0.2,
    random_state=42,
    stratify=df["anx_label"]
)

# 5) HuggingFace Datasets
train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))
test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))

# 6) Tokenization
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
def tokenize(batch):
    toks = tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)
    toks["anx_label"] = batch["anx_label"]
    toks["dep_label"] = batch["dep_label"]
    return toks

train_ds = train_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize,  batched=True)

for ds in (train_ds, test_ds):
    ds.set_format(type="torch",
                  columns=["input_ids","attention_mask","anx_label","dep_label"])

from torch.utils.data import DataLoader

BATCH = 16
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)
eval_loader  = DataLoader(test_ds,  batch_size=BATCH)

# ─── IMPORT LIBRARIES ─────────────────────────────────────────────────────────
import torch.nn as nn
from transformers import BertModel

# ─── DEFINE CUSTOM BERT MODEL FOR ANXIETY AND DEPRESSION CLASSIFICATION ───────
class AnxietyDepressionBERT(nn.Module):
    def __init__(self, pretrained="bert-base-uncased", n_anx=3, n_dep=3):
        super().__init__()
        # Load pre-trained BERT model
        self.bert = BertModel.from_pretrained(pretrained)
        # Hidden size from BERT's configuration
        hid = self.bert.config.hidden_size
        # Dropout layer to prevent overfitting
        self.drop = nn.Dropout(0.3)
        # Linear layer for anxiety classification (output size = number of anxiety classes)
        self.anx = nn.Linear(hid, n_anx)
        # Linear layer for depression classification (output size = number of depression classes)
        self.dep = nn.Linear(hid, n_dep)

    def forward(self, input_ids, attention_mask, anx_label=None, dep_label=None):
        # Forward pass through the BERT encoder
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Apply dropout to the pooled output (CLS token representation)
        pooled = self.drop(out.pooler_output)
        # Pass through anxiety classification head
        a_logits = self.anx(pooled)
        # Pass through depression classification head
        d_logits = self.dep(pooled)

        # Initialize loss as None
        loss = None
        # If labels are provided (during training), calculate loss
        if anx_label is not None and dep_label is not None:
            fn = nn.CrossEntropyLoss()
            # Compute average of anxiety and depression classification losses
            loss = 0.5 * (fn(a_logits, anx_label) + fn(d_logits, dep_label))

        # Return dictionary containing loss (if computed) and raw logits
        return {"loss": loss, "a_logits": a_logits, "d_logits": d_logits}

# ─── IMPORT SCHEDULER ─────────────────────────────────────────────────────────
from transformers import get_linear_schedule_with_warmup

# ─── INITIALIZE MODEL ─────────────────────────────────────────────────────────
# Instantiate the custom AnxietyDepressionBERT model and move it to the specified device (CPU or GPU)
model = AnxietyDepressionBERT().to(device)

# ─── SET UP OPTIMIZER ─────────────────────────────────────────────────────────
# Use AdamW optimizer (recommended for transformers) with a learning rate of 2e-5
optim = torch.optim.AdamW(model.parameters(), lr=2e-5)

# ─── DEFINE SCHEDULER ─────────────────────────────────────────────────────────
# Calculate the total number of training steps (number of batches × epochs)
steps = len(train_loader) * 3  # Assuming 3 epochs

# Create a learning rate scheduler with linear decay and warmup phase (set warmup_steps=0 here)
sched = get_linear_schedule_with_warmup(optim, 0, steps)

import time
from tqdm.auto import tqdm

EPOCHS = 3
for ep in range(1, EPOCHS+1):
    model.train()# ─── IMPORT SCHEDULER ─────────────────────────────────────────────────────────
    from transformers import get_linear_schedule_with_warmup

    # ─── INITIALIZE MODEL ─────────────────────────────────────────────────────────
    # Instantiate the custom AnxietyDepressionBERT model and move it to the specified device (CPU or GPU)
    model = AnxietyDepressionBERT().to(device)

    # ─── SET UP OPTIMIZER ─────────────────────────────────────────────────────────
    # Use AdamW optimizer (recommended for transformers) with a learning rate of 2e-5
    optim = torch.optim.AdamW(model.parameters(), lr=2e-5)

    # ─── DEFINE SCHEDULER ─────────────────────────────────────────────────────────
    # Calculate the total number of training steps (number of batches × epochs)
    steps = len(train_loader) * 3  # Assuming 3 epochs

    # Create a learning rate scheduler with linear decay and warmup phase (set warmup_steps=0 here)
    sched = get_linear_schedule_with_warmup(optim, 0, steps)

    t0 = time.time()
    total = 0
    loop = tqdm(train_loader, desc=f"Epoch {ep}/{EPOCHS}")
    for batch in loop:
        optim.zero_grad()
        batch = {k:v.to(device) for k,v in batch.items()}
        out   = model(batch["input_ids"], batch["attention_mask"],
                      batch["anx_label"], batch["dep_label"])
        out["loss"].backward()
        optim.step(); sched.step()
        total += out["loss"].item()
        loop.set_postfix(loss=total/(loop.n+1))
    print(f"→ Ep {ep} took {time.time()-t0:.1f}s, avg loss {total/len(train_loader):.4f}")

# Import necessary libraries
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Set the model to evaluation mode
model.eval()

# Create empty lists to store predictions and true labels
a_preds, a_labels = [], []
d_preds, d_labels = [], []

# Turn off gradient calculations to save memory and speed things up
with torch.no_grad():
    # Loop through all the batches in the evaluation set
    for batch in eval_loader:
        # Move the batch to the correct device (CPU or GPU)
        batch = {k: v.to(device) for k, v in batch.items()}

        # Pass the input through the model
        out = model(batch["input_ids"], batch["attention_mask"])

        # Get the predicted class for anxiety and depression
        a = out["a_logits"].argmax(dim=1).cpu().numpy()
        d = out["d_logits"].argmax(dim=1).cpu().numpy()

        # Add predictions and true labels to the lists
        a_preds.extend(a)
        a_labels.extend(batch["anx_label"].cpu().numpy())
        d_preds.extend(d)
        d_labels.extend(batch["dep_label"].cpu().numpy())

# Print out evaluation results for anxiety
print("→ BERT Anxiety  —  Accuracy:", accuracy_score(a_labels, a_preds))
print("               Precision:", precision_score(a_labels, a_preds, average="macro"))
print("               Recall   :", recall_score(a_labels, a_preds, average="macro"))
print("               F1-score :", f1_score(a_labels, a_preds, average="macro"))

# Print out evaluation results for depression
print("\n→ BERT Depression  —  Accuracy:", accuracy_score(d_labels, d_preds))
print("                   Precision:", precision_score(d_labels, d_preds, average="macro"))
print("                   Recall   :", recall_score(d_labels, d_preds, average="macro"))
print("                   F1-score :", f1_score(d_labels, d_preds, average="macro"))

# Import the function to generate a full classification report
from sklearn.metrics import classification_report

# Print classification report for anxiety predictions
print("=== BERT Anxiety Classification Report ===")
print(classification_report(
    a_labels, a_preds,
    target_names=["Minimal/None", "Mild", "Severe"],  # Names for the classes
    digits=4  # Show results up to 4 decimal places
))

# Print classification report for depression predictions
print("=== BERT Depression Classification Report ===")
print(classification_report(
    d_labels, d_preds,
    target_names=["Minimal/None", "Mild", "Severe"],  # Names for the classes
    digits=4  # Show results up to 4 decimal places
))

# Import confusion matrix function and plotting libraries
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Create the confusion matrix for anxiety predictions
cm_anx = confusion_matrix(a_labels, a_preds)

# Set the figure size
plt.figure(figsize=(5,4))

# Plot the confusion matrix as a heatmap
sns.heatmap(
    cm_anx, annot=True, fmt="d",
    xticklabels=["Minimal/None", "Mild", "Severe"],  # Class names on x-axis
    yticklabels=["Minimal/None", "Mild", "Severe"]   # Class names on y-axis
)

# Add titles and labels
plt.title("BERT Anxiety Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")

# Show the plot
plt.show()

# Create the confusion matrix for depression predictions
cm_dep = confusion_matrix(d_labels, d_preds)

# Set the figure size
plt.figure(figsize=(5,4))

# Plot the confusion matrix as a heatmap
sns.heatmap(
    cm_dep, annot=True, fmt="d",
    xticklabels=["Minimal/None", "Mild", "Severe"],  # Class names on x-axis
    yticklabels=["Minimal/None", "Mild", "Severe"]   # Class names on y-axis
)

# Add titles and labels
plt.title("BERT Depression Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")

# Show the plot
plt.show()

# ✅ Save the model weights after training and evaluation
torch.save(model.state_dict(), "bert_anx_dep.pth")
print("✅ Model saved to Google Drive as 'bert_anx_dep.pth'")

import torch
import torch.nn as nn
from transformers import BertTokenizerFast, BertModel

# Define your custom model class
class AnxietyDepressionBERT(nn.Module):
    def __init__(self, pretrained="bert-base-uncased", n_anx=3, n_dep=3):
        super().__init__()
        self.bert = BertModel.from_pretrained(pretrained)
        hid = self.bert.config.hidden_size
        self.drop = nn.Dropout(0.3)
        self.anx = nn.Linear(hid, n_anx)
        self.dep = nn.Linear(hid, n_dep)

    def forward(self, input_ids, attention_mask, anx_label=None, dep_label=None):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = self.drop(out.pooler_output)
        a_logits = self.anx(pooled)
        d_logits = self.dep(pooled)

        loss = None
        if anx_label is not None and dep_label is not None:
            fn = nn.CrossEntropyLoss()
            loss = 0.5 * (fn(a_logits, anx_label) + fn(d_logits, dep_label))

        return {"loss": loss, "a_logits": a_logits, "d_logits": d_logits}

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load tokenizer from Hugging Face
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# Instantiate model and load saved weights
model = AnxietyDepressionBERT().to(device)
model.load_state_dict(torch.load("bert_anx_dep.pth", map_location=device))
model.eval()

print("✅ Model and tokenizer loaded — ready for Gradio!")

!pip install Gradio

# Import libraries
import torch
import numpy as np
import pandas as pd
import gradio as gr
from torch.nn.functional import softmax
from transformers import BertTokenizerFast

# ─── 1) Device & Load Model/Tokenizer ─────────────────────────────────────
# Set device to GPU if available, else CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# Load the trained model
model = AnxietyDepressionBERT().to(device)
model.load_state_dict(torch.load("bert_anx_dep.pth", map_location=device))
model.eval()  # Set model to evaluation mode

print("✅ Model and tokenizer loaded — ready for Gradio!")

# ─── 2) Questions and Answer Options ──────────────────────────────────────
# List of anxiety-related questions
anxiety_questions = [
    "How often have you felt nervous, anxious, or on edge due to academic pressure?",
    "How often have you been unable to stop worrying about academic affairs?",
    "How often have you had trouble relaxing because of academic pressure?",
    "How often have you been easily annoyed or irritated because of academic pressure?",
    "How often have you worried too much about academic affairs?",
    "How often have you been so restless due to academic pressure that it is hard to sit still?",
    "How often have you felt afraid, as if something awful might happen?"
]

# List of depression-related questions
depression_questions = [
    "How often have you felt little interest or pleasure in doing things?",
    "How often have you felt down, depressed, or hopeless?",
    "How often have you had trouble falling or staying asleep, or sleeping too much?",
    "How often have you felt tired or had little energy?",
    "How often have you had poor appetite or overeating?",
    "How often have you felt bad about yourself or that you are a failure?",
    "How often have you had trouble concentrating on things?",
    "How often have you moved or spoken so slowly that others noticed?",
    "How often have you thought that you would be better off dead or hurting yourself?"
]

# Dropdown options for each question
options = ["Never", "Rarely", "Sometimes", "Often", "Always"]

# ─── 3) Inference Function ────────────────────────────────────────────────
def classify_from_questions(*responses):
    # Format all responses into a single string
    formatted_text = ""
    for i, ans in enumerate(responses[:7]):  # First 7 are anxiety
        formatted_text += f"A{i+1}:{ans}. "
    for i, ans in enumerate(responses[7:]):  # Next 9 are depression
        formatted_text += f"D{i+1}:{ans}. "

    # Tokenize the formatted string
    toks = tokenizer(
        formatted_text,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).to(device)

    # Run the model to get predictions
    out = model(toks["input_ids"], toks["attention_mask"])
    anx_p = softmax(out["a_logits"], dim=1)[0].cpu().detach().numpy()
    dep_p = softmax(out["d_logits"], dim=1)[0].cpu().detach().numpy()

    # Get the final predicted label
    labels = ["Minimal/None", "Mild", "Severe"]
    anx_lbl = labels[int(anx_p.argmax())]
    dep_lbl = labels[int(dep_p.argmax())]

    # Create a summary text
    summary = (
        f"**Anxiety Severity:** {anx_lbl}  \n"
        f"**Depression Severity:** {dep_lbl}"
    )

    # Create a DataFrame showing probabilities
    df = pd.DataFrame({
        "Anxiety Probability": anx_p,
        "Depression Probability": dep_p
    }, index=labels)

    return summary, df

# ─── 4) Build Gradio UI ───────────────────────────────────────────────────
iface = gr.Interface(
    fn=classify_from_questions,
    inputs=[
        # Create dropdowns for each question
        *[gr.Dropdown(choices=options, label=q) for q in anxiety_questions],
        *[gr.Dropdown(choices=options, label=q) for q in depression_questions]
    ],
    outputs=[
        gr.Markdown(label="Severity Classification"),  # Text output
        gr.Dataframe(label="Probability Table")        # Table output
    ],
    title="Anxiety + Depression Classifier (BERT)",
    description="Answer 7 Anxiety + 9 Depression questions to receive severity classification and class probability breakdown using a fine-tuned BERT model."
)

# Launch the Gradio app
iface.launch()

"""# 📣 Instructions for Marker:

- Please click "Runtime" ➔ "Run All" to generate a fresh Gradio public link.
- A new public link will appear in the output.
- You can open the new link to test the BERT Anxiety + Depression Classifier.
"""