# -*- coding: utf-8 -*-
"""ROBERTa ANXIETY DEPRESSION 42

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUbbos-ZhwklzBFqRcPzfGhJxQr5xkcy
"""

!pip install transformers datasets torch pandas scikit-learn matplotlib seaborn

!pip install datasets

!pip install gradio

from google.colab import drive
drive.mount('/content/drive')

directory_path = "content/drive/MyDrive/EXCEL DATASETS"

# First, let's set up the GPU environment
import torch

# Check if GPU is available and set device
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'Using GPU: {torch.cuda.get_device_name(0)}')
else:
    device = torch.device("cpu")
    print('No GPU available, using CPU instead')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizer, BertConfig
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess data
def load_and_preprocess():
    df = pd.read_csv("/content/drive/MyDrive/EXCEL DATASETS/COMBINED_ANXIETY_DEPRESSION_MAPPED2.csv")

    # Combine text features
    anxiety_cols = [col for col in df.columns if 'ANXIETY' in col and 'LABEL' not in col]
    depression_cols = [col for col in df.columns if 'DEPRESSION' in col and 'LABEL' not in col]

    def create_text(row):
        anxiety_part = " ".join(f"A{i+1}:{row[col]}" for i,col in enumerate(anxiety_cols))
        depression_part = " ".join(f"D{i+1}:{row[col]}" for i,col in enumerate(depression_cols))
        return f"[AGE:{row['AGE']}] [GENDER:{row['GENDER']}] {anxiety_part} {depression_part}"

    df['text'] = df.apply(create_text, axis=1)

    # Verify labels
    print("Label distributions:")
    print("Anxiety:", df['ANXIETY LABEL'].value_counts())
    print("Depression:", df['DEPRESSION LABEL'].value_counts())

    return df

df = load_and_preprocess()

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import RobertaTokenizerFast
from torch.utils.data import DataLoader

# 1) Load your merged CSV
csv_path = "/content/drive/MyDrive/EXCEL DATASETS/COMBINED_ANXIETY_DEPRESSION_MAPPED2.csv"
df = pd.read_csv(csv_path)

# 2) Combine question columns into one text field
meta = ["AGE","GENDER","ANXIETY LABEL","DEPRESSION LABEL"]
qs   = [c for c in df.columns if c not in meta]
df["text"] = df[qs].astype(str).agg(" . ".join, axis=1)

# 3) Rename label columns for convenience
df = df.rename(columns={
    "ANXIETY LABEL":    "anx_label",
    "DEPRESSION LABEL": "dep_label"
})

# 4) Stratified train/test split on anxiety
train_df, test_df = train_test_split(
    df[["text","anx_label","dep_label"]],
    test_size=0.2,
    random_state=42,
    stratify=df["anx_label"]
)

# 5) Convert to Hugging Face Dataset objects
train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))
test_ds  = Dataset.from_pandas(test_df .reset_index(drop=True))

# 6) Tokenize with RoBERTa-large
roberta_tokenizer = RobertaTokenizerFast.from_pretrained("roberta-large")

def tokenize_roberta(batch):
    toks = roberta_tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )
    toks["anx_label"] = batch["anx_label"]
    toks["dep_label"] = batch["dep_label"]
    return toks

train_ds = train_ds.map(tokenize_roberta, batched=True)
test_ds  = test_ds .map(tokenize_roberta, batched=True)

# 7) Set output format for PyTorch
for ds in (train_ds, test_ds):
    ds.set_format(
        type="torch",
        columns=["input_ids","attention_mask","anx_label","dep_label"]
    )

# OptionalWrap in DataLoaders
BATCH_SIZE = 16
train_loader_r = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
eval_loader_r  = DataLoader(test_ds,  batch_size=BATCH_SIZE)

print(f"âœ… RoBERTa-large pipeline ready: {len(train_loader_r)} train batches, {len(eval_loader_r)} eval batches")

from transformers import RobertaTokenizerFast
from datasets import Dataset
from torch.utils.data import DataLoader

# 1) Instantiate RoBERTa-large tokenizer
roberta_tokenizer = RobertaTokenizerFast.from_pretrained("roberta-large")

# 2) Preprocessing fn (shorten to 128 tokens)
def preprocess_roberta(batch):
    toks = roberta_tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128   # lowered from 256
    )
    toks["anx_label"] = batch["anx_label"]
    toks["dep_label"] = batch["dep_label"]
    return toks

# 3) Tokenize your existing train_df/test_df
train_ds_r = Dataset.from_pandas(train_df.reset_index(drop=True)).map(
    preprocess_roberta, batched=True
)
test_ds_r  = Dataset.from_pandas(test_df.reset_index(drop=True)).map(
    preprocess_roberta, batched=True
)

# 4) Set PyTorch tensor format
for ds in (train_ds_r, test_ds_r):
    ds.set_format(
        type="torch",
        columns=["input_ids","attention_mask","anx_label","dep_label"]
    )

# 5) DataLoaders (smaller batch)
BATCH_SIZE = 8
train_loader_r = DataLoader(train_ds_r, batch_size=BATCH_SIZE, shuffle=True)
eval_loader_r  = DataLoader(test_ds_r,  batch_size=BATCH_SIZE)

# Import libraries
import torch.nn as nn
from transformers import RobertaModel

# Define the RoBERTa model for anxiety and depression classification
class AnxietyDepressionRoBERTaLarge(nn.Module):
    def __init__(self, pretrained="roberta-large", n_anx=3, n_dep=3):
        super().__init__()
        # Load pre-trained RoBERTa model
        self.backbone = RobertaModel.from_pretrained(pretrained)
        hid = self.backbone.config.hidden_size

        # Add dropout for regularization (higher dropout rate for bigger model)
        self.drop = nn.Dropout(0.5)

        # Classification head for anxiety (3 classes)
        self.anx = nn.Linear(hid, n_anx)

        # Classification head for depression (3 classes)
        self.dep = nn.Linear(hid, n_dep)

    def forward(self, input_ids, attention_mask, anx_label=None, dep_label=None):
        # Pass inputs through RoBERTa backbone
        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)

        # Apply dropout to pooled output
        pooled = self.drop(out.pooler_output)

        # Get logits from anxiety and depression heads
        anx_logits = self.anx(pooled)
        dep_logits = self.dep(pooled)

        loss = None
        # If labels are given, calculate classification loss
        if anx_label is not None and dep_label is not None:
            fn = nn.CrossEntropyLoss()
            # Average the two losses (anxiety + depression)
            loss = 0.5 * (fn(anx_logits, anx_label) + fn(dep_logits, dep_label))

        # Return loss and logits
        return {"loss": loss, "anx_logits": anx_logits, "dep_logits": dep_logits}

# Instantiate the model and move it to the correct device
model_r = AnxietyDepressionRoBERTaLarge().to(device)

# 4) Freeze all layers except the last 4 transformer layers
for name, param in model_r.backbone.named_parameters():
    if "encoder.layer." in name:
        # Get the layer number from the parameter name
        layer_id = int(name.split("layer.")[1].split(".")[0])

        # Freeze layers except for the last 4 layers
        if layer_id < model_r.backbone.config.num_hidden_layers - 4:
            param.requires_grad = False

# Import libraries
import torch
from transformers import get_linear_schedule_with_warmup
from tqdm.auto import tqdm

# â”€â”€â”€ A) Make sure model is on GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_r = AnxietyDepressionRoBERTaLarge().to(device)

# (Early layers were already frozen before)

# â”€â”€â”€ B) Set up Optimizer & Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EPOCHS = 3

# AdamW optimizer for fine-tuning
optim_r = torch.optim.AdamW(
    model_r.parameters(),
    lr=1e-5,           # Smaller learning rate for pre-trained models
    weight_decay=0.01  # Add some regularization
)

# Total number of steps for scheduler
total_steps = len(train_loader_r) * EPOCHS

# Learning rate scheduler with warmup
sched_r = get_linear_schedule_with_warmup(
    optim_r,
    num_warmup_steps=int(0.1 * total_steps),  # Warm up for 10% of total steps
    num_training_steps=total_steps
)

# â”€â”€â”€ C) Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for epoch in range(1, EPOCHS + 1):
    model_r.train()  # Set model to training mode
    running_loss = 0.0
    pbar = tqdm(train_loader_r, desc=f"RoBERTa Ep {epoch}/{EPOCHS}")  # Progress bar

    for batch in pbar:
        optim_r.zero_grad()  # Reset gradients

        # Move batch data to GPU
        input_ids      = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        anx_labels     = batch["anx_label"].to(device)
        dep_labels     = batch["dep_label"].to(device)

        # Forward pass and calculate loss
        out = model_r(
            input_ids,
            attention_mask,
            anx_labels,
            dep_labels
        )
        loss = out["loss"]

        # Backward pass and update weights
        loss.backward()
        optim_r.step()
        sched_r.step()

        # Update running loss
        running_loss += loss.item()
        pbar.set_postfix(avg_loss=running_loss/(pbar.n+1))

    # Print average loss for the epoch
    avg_loss = running_loss / len(train_loader_r)
    print(f"â†’ [RoBERTa-large] Epoch {epoch} finished. Avg loss: {avg_loss:.4f}")

# Import libraries for evaluation metrics
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Set the model to evaluation mode
model_r.eval()

# Create empty lists to store predictions and true labels
a_preds_r, a_labels_r = [], []
d_preds_r, d_labels_r = [], []

# Turn off gradient calculation during evaluation
with torch.no_grad():
    for batch in eval_loader_r:
        # Move batch data to the correct device
        batch = {k: v.to(device) for k, v in batch.items()}

        # Get model outputs
        out = model_r(batch["input_ids"], batch["attention_mask"])

        # Get predicted class for anxiety and depression
        a = out["anx_logits"].argmax(dim=1).cpu().numpy()
        d = out["dep_logits"].argmax(dim=1).cpu().numpy()

        # Add predictions and true labels to the lists
        a_preds_r.extend(a)
        a_labels_r.extend(batch["anx_label"].cpu().numpy())
        d_preds_r.extend(d)
        d_labels_r.extend(batch["dep_label"].cpu().numpy())

# Print evaluation results for Anxiety
print("â†’ RoBERTa-large Anxiety  â€”  Accuracy:", accuracy_score(a_labels_r, a_preds_r))
print("                           Precision:", precision_score(a_labels_r, a_preds_r, average="macro"))
print("                           Recall   :", recall_score   (a_labels_r, a_preds_r, average="macro"))
print("                           F1-score :", f1_score       (a_labels_r, a_preds_r, average="macro"))

# Print evaluation results for Depression
print("\nâ†’ RoBERTa-large Depression  â€”  Accuracy:", accuracy_score(d_labels_r, d_preds_r))
print("                             Precision:", precision_score(d_labels_r, d_preds_r, average="macro"))
print("                             Recall   :", recall_score   (d_labels_r, d_preds_r, average="macro"))
print("                             F1-score :", f1_score       (d_labels_r, d_preds_r, average="macro"))

# Import function to generate a full classification report
from sklearn.metrics import classification_report

# Print classification report for Anxiety predictions
print("=== RoBERTa-large Anxiety Classification Report ===")
print(classification_report(
    a_labels_r, a_preds_r,
    target_names=["Minimal/None", "Mild", "Severe"],  # Names for each class
    digits=4  # Show 4 decimal places in the output
))

# Print classification report for Depression predictions
print("=== RoBERTa-large Depression Classification Report ===")
print(classification_report(
    d_labels_r, d_preds_r,
    target_names=["Minimal/None", "Mild", "Severe"],  # Names for each class
    digits=4  # Show 4 decimal places in the output
))

# Import confusion matrix function and plotting libraries
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Create the confusion matrix for Anxiety predictions
cm_anx_r = confusion_matrix(a_labels_r, a_preds_r)

# Set the figure size
plt.figure(figsize=(5,4))

# Plot the confusion matrix as a heatmap
sns.heatmap(
    cm_anx_r, annot=True, fmt="d",
    xticklabels=["Minimal/None", "Mild", "Severe"],  # Class names for x-axis
    yticklabels=["Minimal/None", "Mild", "Severe"]   # Class names for y-axis
)

# Add title and labels to the plot
plt.title("RoBERTa-large Anxiety Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")

# Show the plot
plt.show()

# Create the confusion matrix for Depression predictions
cm_dep_r = confusion_matrix(d_labels_r, d_preds_r)

# Set the figure size
plt.figure(figsize=(5,4))

# Plot the confusion matrix as a heatmap
sns.heatmap(
    cm_dep_r, annot=True, fmt="d",
    xticklabels=["Minimal/None", "Mild", "Severe"],  # Class names for x-axis
    yticklabels=["Minimal/None", "Mild", "Severe"]   # Class names for y-axis
)

# Add title and labels to the plot
plt.title("RoBERTa-large Depression Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")

# Show the plot
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# Assuming 'model_r' is your trained AnxietyDepressionRoBERTaLarge model
torch.save(model_r.state_dict(), "/content/drive/MyDrive/roberta_large_anx_dep.pth")
print("âœ… Model saved to Google Drive: roberta_large_anx_dep.pth")

# Import libraries
import torch
import pandas as pd
import numpy as np
import gradio as gr
from torch.nn.functional import softmax
from transformers import RobertaTokenizerFast

# â”€â”€â”€ 1) Device & Load Model/Tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Set device to GPU if available, else CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained RoBERTa tokenizer
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-large")

# Load the fine-tuned RoBERTa model
model = AnxietyDepressionRoBERTaLarge().to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/roberta_large_anx_dep.pth", map_location=device))
model.eval()  # Set model to evaluation mode

print("âœ… RoBERTa-large model and tokenizer loaded â€” ready for Gradio!")

# â”€â”€â”€ 2) Questions and Answer Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# List of anxiety-related questions
anxiety_questions = [
    "How often have you felt nervous, anxious, or on edge due to academic pressure?",
    "How often have you been unable to stop worrying about academic affairs?",
    "How often have you had trouble relaxing because of academic pressure?",
    "How often have you been easily annoyed or irritated because of academic pressure?",
    "How often have you worried too much about academic affairs?",
    "How often have you been so restless due to academic pressure that it is hard to sit still?",
    "How often have you felt afraid, as if something awful might happen?"
]

# List of depression-related questions
depression_questions = [
    "How often have you felt little interest or pleasure in doing things?",
    "How often have you felt down, depressed, or hopeless?",
    "How often have you had trouble falling or staying asleep, or sleeping too much?",
    "How often have you felt tired or had little energy?",
    "How often have you had poor appetite or overeating?",
    "How often have you felt bad about yourself or that you are a failure?",
    "How often have you had trouble concentrating on things?",
    "How often have you moved or spoken so slowly that others noticed?",
    "How often have you thought that you would be better off dead or hurting yourself?"
]

# Dropdown options for each question
options = ["Never", "Rarely", "Sometimes", "Often", "Always"]

# â”€â”€â”€ 3) Inference Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def classify_from_questions(*responses):
    # Format user responses into a single text string
    formatted_text = ""
    for i, ans in enumerate(responses[:7]):  # First 7 responses for anxiety
        formatted_text += f"A{i+1}:{ans}. "
    for i, ans in enumerate(responses[7:]):  # Next 9 responses for depression
        formatted_text += f"D{i+1}:{ans}. "

    # Tokenize the formatted text
    toks = tokenizer(
        formatted_text,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).to(device)

    # Get model predictions
    out = model(toks["input_ids"], toks["attention_mask"])
    anx_p = softmax(out["anx_logits"], dim=1)[0].cpu().detach().numpy()
    dep_p = softmax(out["dep_logits"], dim=1)[0].cpu().detach().numpy()

    # Get the predicted labels
    labels = ["Minimal/None", "Mild", "Severe"]
    anx_lbl = labels[int(anx_p.argmax())]
    dep_lbl = labels[int(dep_p.argmax())]

    # Create a summary text for the user
    summary = (
        f"**Anxiety Severity:** {anx_lbl}  \n"
        f"**Depression Severity:** {dep_lbl}"
    )

    # Create a DataFrame showing probability scores
    df = pd.DataFrame({
        "Anxiety Probability": anx_p,
        "Depression Probability": dep_p
    }, index=labels)

    return summary, df

# â”€â”€â”€ 4) Build Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
iface = gr.Interface(
    fn=classify_from_questions,
    inputs=[
        # Create dropdowns for each question
        *[gr.Dropdown(choices=options, label=q) for q in anxiety_questions],
        *[gr.Dropdown(choices=options, label=q) for q in depression_questions]
    ],
    outputs=[
        gr.Markdown(label="Severity Classification"),  # Shows the result as text
        gr.Dataframe(label="Probability Table")        # Shows probability table
    ],
    title="Anxiety + Depression Classifier (RoBERTa-large)",
    description="Answer 7 Anxiety + 9 Depression questions to receive severity classification and class probability breakdown using a fine-tuned RoBERTa-large model."
)

# Launch the Gradio web app
iface.launch()

"""# ðŸ“£ Instructions for Marker:

- Please click "Runtime" âž” "Run All" to generate a fresh Gradio public link.
- A new public link will appear in the output.
- You can open the new link to test the RORBERTa Anxiety + Depression Classifier.
"""